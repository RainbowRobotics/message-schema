"""
mongo_db.py
"""
import asyncio
import gzip
import os
import time
from collections.abc import (
    Sequence,
)
from datetime import (
    UTC,
    date,
    datetime,
    timedelta,
)
from typing import (
    Annotated,
    Any,
)
from zoneinfo import ZoneInfo

from bson import json_util
from fastapi import (
    Depends,
    FastAPI,
)
from motor.motor_asyncio import (
    AsyncIOMotorClient,
    AsyncIOMotorCollection,
    AsyncIOMotorDatabase,
)
from pymongo import (
    MongoClient,
)
from pymongo.errors import (
    OperationFailure,
    PyMongoError,
)
from rb_modules.log import (
    rb_log,
)
from rb_utils.date import (
    get_current_dt_yyyymmddhhmmss,
)
from rb_utils.parser import (
    _normalize_filter,
)

client: AsyncIOMotorClient | None = None
py_mongo_client: MongoClient | None = None
db: AsyncIOMotorDatabase | None = None


def _norm_key_spec(keys: Sequence[tuple[str, int | str]] | tuple[str, int | str] | str):
    if isinstance(keys, str) or (
        isinstance(keys, tuple) and len(keys) == 2 and isinstance(keys[0], str)
    ):
        key_spec = [keys] if isinstance(keys, tuple) else [(keys, 1)]
    else:
        key_spec = list(keys)
    # 1ì€ 1ë¡œ, ê·¸ ì™¸(ì˜ˆ: "text", -1)ëŠ” ê·¸ëŒ€ë¡œ
    return [(k, 1 if v == 1 else v) for k, v in key_spec], key_spec

async def _find_index_name_by_keys(col, norm_req):
    existing = await col.index_information()
    for idx_name, meta in existing.items():
        ex_key = meta.get("key") or []
        norm_ex = [(k, 1 if v == 1 else v) for k, v in ex_key]
        if norm_ex == norm_req:
            return idx_name, meta
    return None, None

async def ensure_replica_set(
    motor_client: AsyncIOMotorClient,
    *,
    rs_name: str = "rs0",
    seed_host: str = "rrs-mongo-dev:27017",
):
    """
    1) rs.statusë¡œ í™œì„± ì—¬ë¶€ í™•ì¸ (Motor)
    2) NotYetInitializedë©´ ë™ê¸° pymongoë¡œ rs.initiate() 1íšŒ ìˆ˜í–‰
    3) PRIMARY ì„ ì¶œë  ë•Œê¹Œì§€ helloë¡œ ëŒ€ê¸° (Motor)
    """
    # 1) ì´ë¯¸ í™œì„±í™”?
    try:
        await motor_client.admin.command("replSetGetStatus")
        print("âœ… Replica set already active.")
        return
    except OperationFailure as e:
        msg = str(e)
        if "not running with --replSet" in msg:
            print("âŒ mongodê°€ --replSet ì—†ì´ ì‹¤í–‰ë¨. ì»¨í…Œì´ë„ˆ command í™•ì¸ í•„ìš”.")
            return
        if "no replset config has been received" in msg:
            # 2) initiate (ë™ê¸° pymongo ì‚¬ìš©: await ê¸ˆì§€)
            print("ðŸ”§ Initializing replica set...")
            sync_client: MongoClient = MongoClient(
                f"mongodb://{seed_host}"
            )  # RS íŒŒë¼ë¯¸í„° ì—†ì´ ë‹¨ì¼ ì—°ê²°
            try:
                sync_client.admin.command(
                    "replSetInitiate",
                    {"_id": rs_name, "members": [{"_id": 0, "host": seed_host}]},
                )
                print("âœ… rs.initiate() sent")
            except OperationFailure as e2:
                # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° í¬í•¨
                if "already initialized" in str(e2):
                    print("â„¹ï¸ Replica set already initialized (server says).")
                else:
                    raise
            finally:
                sync_client.close()
        else:
            # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ìœ„ë¡œ
            raise

    # 3) PRIMARY ì„ ì¶œ ëŒ€ê¸° (hello: MongoDB 5+)
    for i in range(90):  # ìµœëŒ€ 90ì´ˆ
        try:
            hello = await motor_client.admin.command({"hello": 1})
            if hello.get("isWritablePrimary"):
                print(f"ðŸŽ‰ PRIMARY ready: {hello.get('primary') or seed_host}")
                return
        except Exception:   # pylint: disable=broad-exception-caught
            pass
        print(f"â³ Waiting for PRIMARY election... {i+1}s", flush=True)
        await asyncio.sleep(1)

    raise RuntimeError("Primary not elected within timeout")

async def ensure_index(
    db_: AsyncIOMotorDatabase,
    col_name: str,
    keys: Sequence[tuple[str, int | str]] | tuple[str, int | str] | str,
    *,
    name: str,
    max_retries: int = 5,
    **create_kwargs,
):
    """
    ë™ì‹œì„± ì•ˆì „:
    1) ë¨¼ì € create_index ì‹œë„
    2) ì˜µì…˜ ì¶©ëŒ/ì´ë¦„ ì¶©ëŒì¼ ë•Œë§Œ ì •í™•ížˆ íƒ€ê¹ƒì„ ì°¾ì•„ ë“œë¡­ í›„ ìž¬ìƒì„±
    3) IndexBuildAborted(276)ëŠ” ì§§ê²Œ ëŒ€ê¸°í•˜ê³  ìž¬ì‹œë„(ë°±ì˜¤í”„)
    """
    col = db_[col_name]
    norm_req, raw_spec = _norm_key_spec(keys)

    # ì´ë¯¸ ë™ì¼ í‚¤ì˜ ì¸ë±ìŠ¤ê°€ ìžˆê³  ì˜µì…˜ë„ ë™ì¼í•˜ë©´ skip
    try:
        existing = await col.index_information()
    except Exception as e:
        rb_log.error(f"[index] index_information failed: {e}")
        raise

    matched_name = None
    matched_meta: dict[str, Any] = {}
    for idx_name, meta in existing.items():
        ex_key = meta.get("key") or []
        if [(k, 1 if v == 1 else v) for k, v in ex_key] == norm_req:
            matched_name, matched_meta = idx_name, meta
            break

    if matched_name:
        opts_to_check = [
            "unique",
            "sparse",
            "default_language",
            "weights",
            "expireAfterSeconds",
            "collation",
        ]
        same_opts = all(
            (matched_meta.get(o) == create_kwargs.get(o))
            for o in opts_to_check
            if (o in matched_meta) or (o in create_kwargs)
        )
        if same_opts:
            return

    # ìž¬ì‹œë„ ë£¨í”„
    delay = 0.1
    for attempt in range(1, max_retries + 1):
        try:
            created_name = await col.create_index(raw_spec, name=name, **create_kwargs)
            return
        except OperationFailure as e:
            code = getattr(e, "code", None)
            msg = str(e)

            # 1) ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ê°€ dropIndexesë¥¼ ì´ì„œ ë¹Œë“œ ì¤‘ë‹¨ë¨ â†’ ìž¬ì‹œë„
            if code == 276 or "IndexBuildAborted" in msg or "dropIndexes command" in msg:
                rb_log.warning(
                    f"[index] build aborted by concurrent drop/create on {col_name}.{name}; "
                    f"retrying attempt={attempt}/{max_retries}"
                )
                await asyncio.sleep(delay)
                delay = min(delay * 2, 1.0)
                continue

            # 2) ì˜µì…˜ ì¶©ëŒ / ì´ë¦„ë§Œ ë‹¤ë¥¸ ë™ì¼í‚¤ ì¶©ëŒ â†’ íƒ€ê¹ƒ ì°¾ì•„ ì •í™•ížˆ ë“œë¡­ í›„ ìž¬ìƒì„±
            if code == 85 or "IndexOptionsConflict" in msg or "different name" in msg:
                try:
                    # í˜„ìž¬ ì¡´ìž¬í•˜ëŠ” ë™ì¼ í‚¤/ë‹¤ë¥¸ ì´ë¦„ ì¸ë±ìŠ¤ ì°¾ê¸°
                    target_name, _ = await _find_index_name_by_keys(col, norm_req)
                    if not target_name:
                        existing = await col.index_information()
                        if name in existing:
                            target_name = name

                    if target_name:
                        await asyncio.sleep(0.05)
                        await col.drop_index(target_name)
                        await asyncio.sleep(0.05)

                    created_name = await col.create_index(raw_spec, name=name, **create_kwargs)
                    rb_log.info(
                        f"[index] recreated {col_name}.{created_name} after resolving conflict",
                        disable_db=True,
                    )
                    return
                except Exception as e2:  # pylint: disable=broad-exception-caught
                    rb_log.error(f"[index] conflict resolution failed on {col_name}.{name}: {e2}")
                    # ê²½í•© ê°€ëŠ¥ì„±ì´ ìžˆìœ¼ë‹ˆ í•œ ë²ˆ ë” ìž¬ì‹œë„ ì—¬ì§€ ì œê³µ
                    await asyncio.sleep(delay)
                    delay = min(delay * 2, 1.0)
                    continue

            # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì „íŒŒ
            raise
        except Exception as e:  # pylint: disable=broad-exception-caught
            # ë¹„ì •í˜• ì—ëŸ¬ë„ ì§§ê²Œ ìž¬ì‹œë„ (ë„¤íŠ¸ì›Œí¬ ë“± ì¼ì‹œ ì´ìŠˆ)
            rb_log.warning(
                f"[index] unexpected error on {col_name}.{name}: {e}; retry {attempt}/{max_retries}",
                disable_db=True,
            )
            await asyncio.sleep(delay)
            delay = min(delay * 2, 1.0)
            continue

    raise RuntimeError(f"ensure_index failed on {col_name}.{name} after {max_retries} attempts")

async def init_indexes(db: AsyncIOMotorDatabase):
    """
    [ì¸ë±ìŠ¤ ì´ˆê¸°í™”]
    - ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    - ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    """
    await ensure_index(db, "robots", "name", name="robots_name_idx", unique=True)
    await ensure_index(
        db,
        "state_logs",
        [("swName", 1), ("level", 1), ("createdAt", -1)],
        name="state_logs_sw_level_created_idx",
    )
    await ensure_index(db, "state_logs", [("contents", "text")], name="state_logs_text_idx")
    await ensure_index(
        db,
        "state_logs",
        [("createdAtDt", 1)],
        name="state_logs_createdAtDt_ttl",
        expireAfterSeconds=60 * 60 * 24 * 90,  # 90ì¼
    )
    await ensure_index(db, "programs", [("name", 1)], name="uniq_program_name", unique=True)


async def init_db(app: FastAPI, uri: str, db_name: str):
    """
    [DB ì´ˆê¸°í™”]
    - ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    - ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    """
    global client, db, py_mongo_client
    client = AsyncIOMotorClient(uri, uuidRepresentation="standard")
    py_mongo_client = MongoClient(uri)
    db = client[db_name]

    await ensure_replica_set(client)
    await init_indexes(db)

    app.state.mongo_client = client
    app.state.mongo_db = db

async def close_db(app: FastAPI):
    """
    [DB ì¢…ë£Œ]
    - ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    - ë°ì´í„°ë² ì´ìŠ¤ ì¢…ë£Œ
    """
    c = app.state.mongo_client

    if c:
        c.close()


async def wait_db_ready(timeout: int = 15):
    """
    [DB ì¤€ë¹„ ëŒ€ê¸°]
    - ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    - ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°
    - ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
    - timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ê¸°ë³¸ 15ì´ˆ)
    """
    start = time.monotonic()
    while db is None:
        print("ðŸ”Ž wait db ready", flush=True)
        if time.monotonic() - start > timeout:
            raise RuntimeError("MongoDB not ready")
        await asyncio.sleep(0.05)

async def get_db():
    """
    [DB ì¡°íšŒ]
    - ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    - ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°
    - ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
    """
    await wait_db_ready()

    if db is None:
        raise RuntimeError("Database not initialized after waiting")

    return db



# === Yujin Added ===

async def add_collection(_db: AsyncIOMotorDatabase, col_name: str) -> bool:
    """
    [ì»¬ë ‰ì…˜ ìƒì„±]
    - ì´ë¯¸ ì¡´ìž¬í•˜ë©´ íŒ¨ìŠ¤
    """
    if col_name is None:
        return False

    await wait_db_ready()

    names = await _db.list_collection_names()
    if col_name not in names:
        await _db.create_collection(col_name)
        return True

    return False

async def get_collection_stats(_db: AsyncIOMotorDatabase, col_name: str, scale: int = 1):
    """
    [ì»¬ë ‰ì…˜ ìƒíƒœ ì¡°íšŒ]
    - ì»¬ë ‰ì…˜ ì´ë¦„ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
    - scale: ë°”ì´íŠ¸->KB, MB ë“± ë‹¨ìœ„ ìŠ¤ì¼€ì¼ (ê¸°ë³¸ byte, KB ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ë ¤ë©´ scale=1024)
    """
    if col_name is None:
        return None

    await wait_db_ready()

    return await _db.command({"collStats": col_name, "scale": scale})

async def archive_collection(
    col: AsyncIOMotorCollection,
    cutoff_utc: date | datetime,
    out_dir: str,
    dry_run: bool = False,
    tz_name: str = "Asia/Seoul"
    ) -> list[dict]:
    """
    [ì•„ì¹´ì´ë¸Œ + dbì‚­ì œ]
    - ì¼ì • ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ì€ ì „ë¶€ ë‚ ì§œë³„ë¡œ ì••ì¶•ë³´ê´€ + dbì—ì„œ ì‚­ì œ
    - ë‚´ë¶€ì ìœ¼ë¡œë§Œ ì‚¬ìš©
    - DB ë‚´ ì €ìž¥ëœ ë°ì´í„°ì˜ timezoneì€ UTCì´ì§€ë§Œ ì €ìž¥ë˜ëŠ” ë°ì´í„°ëŠ” KST ê¸°ì¤€ìœ¼ë¡œ ìžë¦„
        - cutoff_utc : ì•„ì¹´ì´ë¸Œ í•  ë‚ ì§œ ê¸°ì¤€ UTC(í•´ë‹¹ë‚ ì§œ ì´ì „ì˜ ë¡œê·¸ë¥¼ ì „ë¶€ ì•„ì¹´ì´ë¸Œ + ì‚­ì œ)
        - out_dir : ì €ìž¥ë””ë ‰í† ë¦¬. /data/{server}/archive/{service}ë¡œ í†µì¼
        - filters : ê²€ìƒ‰ í•„í„°.
    """
    pipeline = [
        {"$match": {"createdAt": {"$lt": cutoff_utc}}},
        {"$project": {
            "_id": 0,
            "day": {
                "$dateToString": {
                    "date": "$createdAt",
                    "timezone": tz_name,
                    "format": "%Y-%m-%d"
                }
            }
        }},
        {"$group": {"_id": "$day", "count": {"$sum": 1}}},
        {"$sort": {"_id": 1}},
    ]

    print(f"[archive_collection] pipeline: {pipeline}")

    # 3) ë°ì´í„° ì¡°íšŒ
    day_rows = await col.aggregate(pipeline, allowDiskUse=True).to_list(length=None)
    if not day_rows:
        return []

    os.makedirs(out_dir, exist_ok=True)
    results: list[dict] = []

    hint_opt = None
    try:
        idxinfo = await col.index_information()
        if "createdAt_1" in idxinfo:
            hint_opt = "createdAt_1"
    except PyMongoError as e:
        rb_log.error(f"[archive_collection] index_information failed: {e}")

    for row in day_rows:
        day = row["_id"]        # 'YYYY-MM-DD'
        count_est = int(row.get("count", 0))

        y, m, d = map(int, day.split("-"))
        start_kst = datetime(y, m, d, 0, 0, 0, tzinfo=ZoneInfo(tz_name))
        end_kst   = start_kst + timedelta(days=1)
        start_utc = start_kst.astimezone(UTC)
        end_utc   = end_kst.astimezone(UTC)

        fname = f"{day}.ndjson.gz"
        tmp_path = os.path.join(out_dir, fname + ".tmp")
        final_path = os.path.join(out_dir, fname)

        archived_count = 0
        gz_bytes = 0
        deleted_count = 0

        day_filter = {"createdAt": {"$gte": start_utc, "$lt": end_utc}}

        if not dry_run:
            try:
                cursor = col.find(
                    day_filter,
                    projection=None,
                    batch_size=10_000,
                    sort=[("_id", 1)],
                    hint=hint_opt
                )

                print("path : ", tmp_path, final_path)
                with gzip.open(tmp_path, mode="wt", encoding="utf-8") as gz:
                    async for doc in cursor:
                        gz.write(json_util.dumps(doc))
                        gz.write("\n")
                        archived_count += 1

                gz_bytes = os.path.getsize(tmp_path)
                os.replace(tmp_path, final_path)

                if archived_count > 0:
                    delete_kwargs = {}
                    if hint_opt:
                        delete_kwargs["hint"] = hint_opt
                    delete_result = await col.delete_many(day_filter, **delete_kwargs)
                    deleted_count = delete_result.deleted_count

            except PyMongoError as e:
                try:
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)
                except OSError:
                    pass
                results.append({
                    "day": day,
                    "estimatedDocs": count_est,
                    "archivedDocs": archived_count,
                    "deletedDocs": deleted_count,
                    "file": None,
                    "size": None,
                    "error": repr(e)
                })
                continue

        results.append({
            "day": day,
            "estimatedDocs": count_est,
            "archivedDocs": archived_count if not dry_run else 0,
            "deletedDocs": deleted_count if not dry_run else 0,
            "file": None if dry_run else (final_path if archived_count > 0 else None),
            "size": None if dry_run else (gz_bytes if archived_count > 0 else None),
        })

    return results

async def export_collection(
    col: AsyncIOMotorCollection,
    start_utc: datetime | None = None,
    end_utc: datetime | None = None,
    out_dir: str | None = None,
    file_name: str | None = None,
    filters: str | dict[str, Any] | None = None,
    fields: dict[str, Any] | None = None,
    search_text: str | None = None,
    sort: list[tuple[str, int]] | None = None,
    order: str | None = None,
) -> dict:
    """
    [í•„í„° ì•„ì¹´ì´ë¸Œ]
    - ì™¸ë¶€ì ìœ¼ë¡œë„ ì‚¬ìš©
    - í•„í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DBì—ì„œ ë½‘ì•„ë‚¸ ë¡œê·¸ë“¤ì„ íŒŒì¼ë¡œ ì••ì¶• ì €ìž¥ -> ì´í›„ ë°ì´í„°ì „ì†¡ ë“±ìœ¼ë¡œ ì‚¬ìš©
    - DB ë‚´ ì €ìž¥ëœ ë°ì´í„°ì˜ timezoneì€ UTCì´ì§€ë§Œ ì €ìž¥ë˜ëŠ” ë°ì´í„°ëŠ” KST ê¸°ì¤€ìœ¼ë¡œ ìžë¦„
        - out_dir : ì €ìž¥ë””ë ‰í† ë¦¬. /data/{server}/archive/{service}ë¡œ í†µì¼
        - file_name : íŒŒì¼ ì´ë¦„. ìž…ë ¥ ì‹œ ì‚¬ìš©
        - filters : ê²€ìƒ‰ í•„í„°. ê° ì»¬ëŸ¼ì˜ ê²€ìƒ‰ì–´ë¼ë˜ê°€ ë‚ ì§œì‹œê°„ ë“±
    """
    print("========================================")
    # 1) í•„í„° íŒŒì‹±
    filters = _normalize_filter(filters)
    tz_name = "Asia/Seoul"

    # 2) ì¡°íšŒ ì¡°ê±´ êµ¬ì„±
    created_cond: dict[str, Any] = {}
    if start_utc is not None:
        created_cond["$gte"] = start_utc
    if end_utc is not None:
        created_cond["$lt"] = end_utc

    # 3) í•„ë“œ ì„¸íŒ…(_id ì œì™¸í•˜ëŠ” ê²ƒ ì¼ê´„ë¡œ ì¶”ê°€)
    if fields is None:
        fields = {"_id": 0}
    else:
        fields = {**fields, "_id": 0}

    # 4) í…ìŠ¤íŠ¸ ê²€ìƒ‰(q) -> checkDBì—ì„œ ë“±ë¡í•œ ì¸ë±ìŠ¤ì•ˆì—ì„œ ê²€ìƒ‰
    if search_text:
        filters["$text"] = {"$search": search_text}

    # 5) ì •ë ¬: ì•ˆì • ì •ë ¬ì„ ìœ„í•´ ë³´ì¡°í‚¤ë¡œ _idë„ í¬í•¨
    sort_spec = [(sort, order)]
    if sort != "_id":
        sort_spec.append(("_id", order))

    if created_cond:
        filters["createdAt"] = created_cond

    # 3) ê²½ë¡œ ìƒì„±
    os.makedirs(out_dir, exist_ok=True)

    # 4) íŒŒì¼ ì´ë¦„ ê²°ì •
    if not file_name:
        file_name = get_current_dt_yyyymmddhhmmss(tz_name)
    if not file_name.endswith(".ndjson.gz"):
        file_name = f"{file_name}.ndjson.gz"

    tmp_path = os.path.join(out_dir, f"{file_name}.tmp")
    final_path = os.path.join(out_dir, file_name)

    # 5) ë°ì´í„° ì˜ˆìƒ ì¡°íšŒ
    # results: list[dict] = []
    archived_count = 0
    gz_bytes = 0


    print("filters: ", filters)
    count_est = await col.count_documents(filters)
    print(f"[export_collection] count_est: {count_est}")
    if count_est == 0:
        return {
            "estimatedDocs": 0,
            "archivedDocs": 0,
            "file": None,
            "size": None,
        }

    # 6) ë°ì´í„° ì¡°íšŒ ë° ë°˜í™˜
    try:
        cursor = col.find(filters,fields).sort(sort_spec)
        with gzip.open(tmp_path, mode="wt", encoding="utf-8") as gz:
            async for doc in cursor:
                gz.write(json_util.dumps(doc))
                gz.write("\n")
                archived_count += 1
        print(f"[export_collection] archived_count: {archived_count}")
        gz_bytes = os.path.getsize(tmp_path)
        os.replace(tmp_path, final_path)

        return {
            "estimatedDocs": count_est,
            "archivedDocs": archived_count,
            "file": final_path if archived_count > 0 else None,
            "size": gz_bytes if archived_count > 0 else None,
            "error": None,
        }

    except PyMongoError as e:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except OSError:
            pass
        print("ERRORRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR")
        print(f"[export_collection] error: {e}")
        return {
            "estimatedDocs": count_est,
            "archivedDocs": archived_count,
            "file": None,
            "size": None,
            "error": repr(e),
        }




MongoDB = Annotated[AsyncIOMotorDatabase, Depends(get_db)]
MongoCollection = AsyncIOMotorCollection
